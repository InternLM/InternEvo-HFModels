--- huggingface_model/Yi/Yi_6B/modeling_yi.py	2024-09-13 16:45:55.823046000 +0800
+++ huggingface_model/Yi/Yi_6B/modeling_yi.py	2024-09-13 15:56:05.669004000 +0800
@@ -23,6 +23,11 @@
     replace_return_docstrings,
 )
 
+from internlm.core.context import ParallelMode
+from internlm.core.context import global_context as gpc
+from internlm.model.ops.attention import isp_flash_attn_varlen_func, isp_flash_attn_func
+from internlm.initialize.initialize_tensor import normal_, scaled_init_method_normal
+
 from .configuration_yi import YiConfig
 
 
@@ -226,6 +231,12 @@
         bsz, q_len, _ = hidden_states.size()
         flash_attn_available = is_flash_attn_2_available()
 
+        use_packed_dataset = gpc.config.data.get("use_packed_dataset", False)
+        if use_packed_dataset:
+            assert bsz == 1, "hidden_states should be packed into bsz=1 when use_packed_dataset=True"
+            cu_seqlens = gpc.config.data[f"cu_seqlens_data_rank{gpc.get_local_rank(ParallelMode.DATA)}"]
+            max_seqlen = gpc.config.data[f"max_seqlen_data_rank{gpc.get_local_rank(ParallelMode.DATA)}"]
+
         query_states = self.q_proj(hidden_states).view(
             bsz, q_len, self.num_heads, self.head_dim
         )
@@ -268,9 +279,31 @@
         past_key_value = (key_states, value_states) if use_cache else None
 
         if flash_attn_available:
-            attn_output = flash_attn_func(
-                query_states, key_states, value_states, dropout_p=0.0, causal=True
-            )
+            # attn_output = flash_attn_func(
+            #     query_states, key_states, value_states, dropout_p=0.0, causal=True
+            # )
+            if use_packed_dataset:
+                attn_output = isp_flash_attn_varlen_func(
+                    query_states,
+                    key_states,
+                    value_states,
+                    cu_seqlens,
+                    cu_seqlens,
+                    max_seqlen,
+                    max_seqlen,
+                    attention_dropout=0.0,
+                    softmax_scale=None,
+                    causal=False,
+                )
+            else:
+                attn_output = isp_flash_attn_func(
+                    query_states, 
+                    key_states, 
+                    value_states, 
+                    attention_dropout=0.0, 
+                    softmax_scale=None, 
+                    causal=False,
+                )
         else:
             attn_weights = torch.matmul(
                 query_states, key_states.transpose(2, 3)
@@ -897,6 +930,31 @@
             )
         return reordered_past
 
+    def reset_parameters(self, std=0.02):
+        def reset_attn_parameters(layer_idx, layer, use_scaled_init=True, std=0.02):
+            for name, param in layer.self_attn.named_parameters():
+                if param.ndim == 1:  # bias
+                    param.data.zero_()
+                elif "q_proj" in name or "k_proj" in name or "v_proj" in name:  # wq, wk, wv
+                    normal_(std=std)(param.data)
+                elif use_scaled_init:  # wo
+                    scaled_init_method_normal(sigma=std, num_layers=layer_idx + 1)(param.data)
+                else:  # wo
+                    normal_(std=std)(param.data)
+
+            for name, param in layer.mlp.named_parameters():
+                if use_scaled_init:
+                    scaled_init_method_normal(sigma=std, num_layers=layer_idx + 1)(param.data)
+                else:
+                    normal_(std=std)(param.data)
+        with torch.no_grad():
+            for _, param in self.model.embed_tokens.named_parameters():
+                normal_(std=std)(param)
+            for layer_idx, layer in enumerate(self.model.layers):
+                reset_attn_parameters(layer_idx, layer)
+            for _, param in self.lm_head.named_parameters():
+                normal_(std=std)(param)
+
 
 @add_start_docstrings(
     """
